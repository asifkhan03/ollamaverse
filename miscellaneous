kubectl port-forward svc/ollamaverse-py-service 8000:8000 -n aiops


To delete any pods/service from any namsepace we should write
kubectl delete deployment ollamaverse-frontend -n default
kubectl delete service ollamaverse-frontend -n default
not the Pods or service name.


{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::936379345511:oidc-provider/oidc.eks.ap-south-1.amazonaws.com/id/B74468CB963EAD123C5025A03E42B838"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "oidc.eks.ap-south-1.amazonaws.com/id/B74468CB963EAD123C5025A03E42B838:sub": "system:serviceaccount:kube-system:aws-load-balancer-controller",
          "oidc.eks.ap-south-1.amazonaws.com/id/B74468CB963EAD123C5025A03E42B838:aud": "sts.amazonaws.com"
        }
      }
    }
  ]
}


--------------------------------------------------------------------------------------------------------------------------------------------------

https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.7.2/docs/install/iam_policy.json
--------------------------------------------------------------------------------------------------------------------------------------------------


helm repo add eks https://aws.github.io/eks-charts
helm repo update
-------------------


============+++++++++++++++++++++=====================++++++++++++++++++++++++++++=
$ cat alb-values.yaml 
clusterName: aiops-cluster
region: ap-south-1
vpcId: vpc-0a8fe13ef9d0d60c3

serviceAccount:
  create: false
  name: aws-load-balancer-controller

ingressClass: alb

+++++++++++----------------+++++++++++++===================


helm upgrade aws-load-balancer-controller \
    eks/aws-load-balancer-controller \
    -n kube-system \
    -f alb-values.yaml



    ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

helm uninstall aws-load-balancer-controller -n kube-system

helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=ollamaverse-cluster \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller \
  --set region=ap-south-1 \
  --set vpcId=vpc-0a8fe13ef9d0d60c3   








aws ecr get-login-password --region ap-south-1 | docker login --username AWS --password-stdin 976193257685.dkr.ecr.ap-south-1.amazonaws.com



  
docker build -t ollamaverse-backend:2.5 .
docker tag ollamaverse-backend:2.5 976193257685.dkr.ecr.ap-south-1.amazonaws.com/ollamaverse-backend:2.5
docker push 976193257685.dkr.ecr.ap-south-1.amazonaws.com/ollamaverse-backend:2.5




docker build -t ollamaverse-py-backend:2.2 .
docker tag ollamaverse-py-backend:2.2 976193257685.dkr.ecr.ap-south-1.amazonaws.com/ollamaverse-py-backend:2.2
docker push 976193257685.dkr.ecr.ap-south-1.amazonaws.com/ollamaverse-py-backend:2.2
  

docker build -t ollamaverse-frontend:2.2 .
docker tag ollamaverse-frontend:2.2 976193257685.dkr.ecr.ap-south-1.amazonaws.com/ollamaverse-frontend:2.2
docker push 976193257685.dkr.ecr.ap-south-1.amazonaws.com/ollamaverse-frontend:2.2




====================++++++++++++++++++++++=====================++++++++++++++++++++

SETUP EKS DashBoard: 

kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.0/aio/deploy/recommended.yaml


eks-admin-service-account.yaml

apiVersion: v1
kind: ServiceAccount
metadata:
  name: eks-admin
  namespace: kubernetes-dashboard
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: eks-admin
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: eks-admin
  namespace: kubernetes-dashboard



kubectl create token eks-admin -n kubernetes-dashboard





kubernetes-dashboard-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kubernetes-dashboard-ingress
  namespace: kubernetes-dashboard
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/subnets: subnet-01c4e1f0030c41d14,subnet-0676b3c3d8a01d979,subnet-07be6dad2f533444a
    alb.ingress.kubernetes.io/target-type: ip
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:ap-south-1:976193257685:certificate/044c0d59-3c44-4893-8957-ef084947b4da
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTPS":443}]'
    alb.ingress.kubernetes.io/ssl-redirect: '443'
    alb.ingress.kubernetes.io/backend-protocol: 'HTTPS'
    alb.ingress.kubernetes.io/healthcheck-path: /dashboard/
    external-dns.alpha.kubernetes.io/hostname: eks.asifahmadkhan.com
spec:
  ingressClassName: alb
  rules:
    - host: eks.asifahmadkhan.com
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: kubernetes-dashboard
                port:
                  number: 443
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
=====================================================================================================

Full flow diagram:

[Frontend React] 
      |
      | HTTPS POST /auth/login
      v
[Backend Express Node.js]  ----------------------------+
      |                                                  |
      | /auth/signup, /auth/login, /health, /ollama/ask  |
      |                                                  |
      v                                                  |
[Redis Cache] <------ Optional for caching user info     |
      ^                                                  |
      |                                                  |
[MongoDB]  <-- User credentials & data                   |
      |                                                  |
      v                                                  |
[Python Ollama Service (Flask)]  <---------------------+
      | /ask, /models, /health
      |
      | Internal call to Ollama model pods
      v
[Ollama model containers (smollm2 / tinyllama)]



+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
=====================================================================================================

Flow explanation:

Login / Signup:
Frontend → Backend /auth/login or /auth/signup
Backend checks Redis cache, else MongoDB
Password verified → JWT returned

Chat with model:
Frontend → Backend /ollama/ask with JWT
Backend verifies token
Backend forwards prompt to Python service (internal ClusterIP)
Python service calls Ollama model pods (smollm2/tinyllama)
Model generates response → Python service → Backend → Frontend

Health checks:
Kubernetes probes hit /health endpoints for backend and Python service
ALB monitors service readiness

Ingress / ALB:
ALB routes traffic:

      aiops.asifahmadkhan.com → frontend
      api.asifahmadkhan.com → backend
      ollama.asifahmadkhan.com → Python service (only needed for external access)




+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
=====================================================================================================

Key DevOps notes you should know

Internal vs External URLs:
Always prefer internal ClusterIP service URLs for pod-to-pod communication.
Use public ALB only for external frontend traffic.

Probes / readiness:
Readiness probe failing → pod is not considered ready → 503 from ALB.
Make sure /health or /models endpoints return 200 quickly.

Environment variables:
Keep JWT_SECRET consistent across backend & Python (if Python ever verifies JWT)
REDIS_URL → internal service
PYTHON_SERVICE → internal service

Timeouts:
Backend calls Python with 30s timeout
Python calls Ollama model with 120s timeout
Ensure probes/ALB healthchecks don’t timeout

Logging:
Backend logs incoming request → token verification → Python call → response
Python logs prompt → model call → response → return
This helps debug 503/timeout issues


